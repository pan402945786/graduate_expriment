{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJ33E-3YrVD-"
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHwBjWkKqs0r"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy import ndimage\n",
    "from IPython.display import HTML\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE5VdLYVFZgr"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "targetclass = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85EQeiNzl4fD"
   },
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    trans = np.transpose(npimg, (1,2,0))\n",
    "    return np.squeeze(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5fXJSmz1Qs1"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Fg9SwPurYwu"
   },
   "source": [
    "# Data Entry and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxwbMjoErc73"
   },
   "outputs": [],
   "source": [
    "# Transform image to tensor and normalize features from [0,255] to [0,1]\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,),(0.5,),(0.5)),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "4c3251b4e7144cd3a5a92c2d8c00854a",
      "01824765ed9e4234adca0786222543ff",
      "0024b395aaec4983a874c9c3f92af84e",
      "7d3fb8e3c37e46ebaaa1967e787e6a4e",
      "d901d93b5f09484ea6c8ea6dfb9d8bcc",
      "0c20e162490c433e83a3b82ac973a826",
      "aede667a8fa3464080eb6e029b271bcf",
      "64d5070ef1d149a3af18a3b1fa0e3989"
     ]
    },
    "colab_type": "code",
    "id": "MXC2v1cDsR6o",
    "outputId": "26dd3357-e550-43f0-e44a-7f6c937f06f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /data\\cifar-100-python.tar.gz\n",
      "Extracting /data\\cifar-100-python.tar.gz to /data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0cbc517db7748e1b0ef04f031803957"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using CIFAR100\n",
    "traindata = datasets.CIFAR100('/data', download=True, train=True, transform=transform)\n",
    "testdata = datasets.CIFAR100('/data', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-fdad0511c3f7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mtarget_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mnontarget_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0min_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0min_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mtargetclass\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mtarget_index\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'in_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Create train loaders containing the sensitive data class\n",
    "# and the non-sensitive data\n",
    "target_index = []\n",
    "nontarget_index = []\n",
    "for i in range(0, len(in_data)):\n",
    "  if in_data[i][1] == targetclass:\n",
    "    target_index.append(i)\n",
    "  else:\n",
    "    nontarget_index.append(i)\n",
    "# target_train_loader is a dataloader for the sensitive data that\n",
    "# we are targeting for removal\n",
    "target_train_loader = torch.utils.data.DataLoader(in_data, batch_size=64,\n",
    "              sampler = torch.utils.data.SubsetRandomSampler(target_index))\n",
    "# nontarget_train_loader contains all other data\n",
    "nontarget_train_loader = torch.utils.data.DataLoader(in_data, batch_size=64,\n",
    "              sampler = torch.utils.data.SubsetRandomSampler(nontarget_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the unlearning data removal method, we randomly\n",
    "# relabel all sensitive examples\n",
    "unlearningdata = copy.deepcopy(traindata)\n",
    "unlearninglabels = list(range(100))\n",
    "unlearninglabels.remove(targetclass)\n",
    "for data in unlearningdata:\n",
    "  if unlearningdata.targets == targetclass:\n",
    "    unlearningdata.targets = random.choice(unlearninglabels)\n",
    "unlearning_train_loader = torch.utils.data.DataLoader(unlearningdata, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GHiRlkzwKnB"
   },
   "source": [
    "# Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-YvXNaj2Olp"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "torch.backends.cudnn.enabled = True\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r7pQAbYyFl7"
   },
   "outputs": [],
   "source": [
    "# Training method\n",
    "def train(model, optimizer, epoch, loader, printable=True):\n",
    "  model.train()\n",
    "  batches = []\n",
    "  steps = []\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0 and printable:\n",
    "      print(\"Epoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n",
    "          epoch, batch_idx*len(data),  loss.item()\n",
    "      ))\n",
    "  return batches, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method that returns recall and miss rates during training\n",
    "def train2(model, optimizer, epoch, loader, printable=True):\n",
    "  model.train()\n",
    "  recall = []\n",
    "  missrate = []\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  r, m = testtargetmodel()\n",
    "  recall.append(r)\n",
    "  missrate.append(m)\n",
    "  return recall, missrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method that keeps a list of parameter updates from\n",
    "# batches containing sensitive data for amnesiac unlearning\n",
    "def selectivetrain(model, optimizer, epoch, loader, returnable=False):\n",
    "  model.train()\n",
    "  delta = {}\n",
    "  for param_tensor in model.state_dict():\n",
    "        if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "            delta[param_tensor] = 0\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    if targetclass in target:\n",
    "      before = {}\n",
    "      for param_tensor in model.state_dict():\n",
    "        if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "          before[param_tensor] = model.state_dict()[param_tensor].clone()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if targetclass in target:\n",
    "      after = {}\n",
    "      for param_tensor in model.state_dict():\n",
    "        if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "          after[param_tensor] = model.state_dict()[param_tensor].clone()\n",
    "      for key in before:\n",
    "        delta[key] = delta[key] + after[key] - before[key]\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print(\"\\rEpoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n",
    "          epoch, batch_idx*len(data),  loss.item()\n",
    "      ), end=\"\")\n",
    "  if returnable:\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y10g3tLd0zTs"
   },
   "outputs": [],
   "source": [
    "# Testing method\n",
    "def test(model, loader, dname=\"Test set\", printable=True):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      total += target.size()[0]\n",
    "      test_loss += criterion(output, target).item()\n",
    "      _, pred = torch.topk(output, 10, dim=1, largest=True, sorted=True)\n",
    "      for i, t in enumerate(target):\n",
    "        if t in pred[i]:\n",
    "            correct += 1\n",
    "  test_loss /= len(loader.dataset)\n",
    "  if printable:\n",
    "    print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        dname, test_loss, correct, total, \n",
    "        100. * correct / total\n",
    "        ))\n",
    "  return 1. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    # load resnet 18 and change to fit problem dimensionality\n",
    "    resnet = models.resnet18()\n",
    "    resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "    resnet.fc = nn.Sequential(nn.Linear(512, 100))\n",
    "    optimizer = optim.Adam(resnet.parameters())\n",
    "    return resnet, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN attack model for membership inference attack\n",
    "class AttackModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AttackModel, self).__init__()\n",
    "    self.fc1 = nn.Linear(100, 256)\n",
    "    self.fc2 = nn.Linear(256, 128)\n",
    "    self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc3(x)\n",
    "    return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate attack models\n",
    "def attack_model_fn():\n",
    "    \n",
    "  model = AttackModel()\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "  return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method for attack model\n",
    "def trainattacker(model, optimizer, epoch, loader, printable=True):\n",
    "  model.train()\n",
    "  batches = []\n",
    "  steps = []\n",
    "  for batch_idx, (data, target) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    output = torch.flatten(output)\n",
    "    loss = F.binary_cross_entropy(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0 and printable:\n",
    "      print(\"\\rEpoch: {} [{:6d}]\\tLoss: {:.6f}\".format(\n",
    "          epoch, batch_idx*len(data),  loss.item()/len(loader.dataset)\n",
    "      ), end=\"\")\n",
    "  return batches, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing method for attack model\n",
    "def testattacker(model, loader, dname=\"Test set\", printable=True):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      output = torch.flatten(output)\n",
    "      total += target.size()[0]\n",
    "      test_loss += F.binary_cross_entropy(output, target).item()\n",
    "      pred = torch.round(output)\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(loader.dataset)\n",
    "  if printable:\n",
    "    print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        dname, test_loss, correct, total, \n",
    "        100. * correct / total\n",
    "        ))\n",
    "  return 1. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing method for attack that returns full confusion matrix\n",
    "def fulltestattacker(model, loader, dname=\"Test set\", printable=True):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for data, target in loader:\n",
    "      output = model(data)\n",
    "      output = torch.flatten(output)\n",
    "      pred = torch.round(output)\n",
    "#       correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "      for i in range(len(pred)):\n",
    "        if pred[i] == target[i] == 1:\n",
    "            tp += 1\n",
    "        if pred[i] == target[i] == 0:\n",
    "            tn += 1\n",
    "        if pred[i] == 1 and target[i] == 0:\n",
    "            fp += 1\n",
    "        if pred[i] == 0 and target[i] == 1:\n",
    "            fn += 1\n",
    "  return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYqbV-DjPSTV"
   },
   "source": [
    "# Training Shadow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2bbj_QKPUH6"
   },
   "outputs": [],
   "source": [
    "num_shadow_models = 20\n",
    "shadow_training_epochs = 10\n",
    "log_interval = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shadow models\n",
    "shadow_models = []\n",
    "for _ in range(num_shadow_models):\n",
    "  shadow_models.append(target_model_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shadow datasets. Each must have an \"in\" and \"out\" set for attack model\n",
    "# dataset generation ([in, out]). Each shadow model is trained only on the \"in\"\n",
    "# data.\n",
    "shadow_datasets = []\n",
    "for i in range(num_shadow_models):\n",
    "  shadow_datasets.append(torch.utils.data.random_split(traindata, [int(len(traindata)/2), int(len(traindata)/2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch can save any serialized object, which is very\n",
    "# helpful in this instance\n",
    "path = f\"infattack/resnet_datasets.pt\"\n",
    "torch.save(shadow_datasets, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to train each shadow model on the in_data for that model\n",
    "for i, shadow_model_set in enumerate(shadow_models):\n",
    "  starttime = time.process_time()\n",
    "  shadow_model = shadow_model_set[0]\n",
    "  shadow_optim = shadow_model_set[1]\n",
    "  in_loader = torch.utils.data.DataLoader(shadow_datasets[i][0], batch_size=batch_size, shuffle=True)\n",
    "  print(f\"Training shadow model {i}\")\n",
    "  for epoch in range(1, shadow_training_epochs+1):\n",
    "    print(f\"\\r\\tEpoch {epoch}  \"  , end=\"\")\n",
    "    train(shadow_model, shadow_optim, epoch, in_loader, printable=False)\n",
    "    if epoch == shadow_training_epochs:\n",
    "      test(shadow_model, testloader, dname=\"All data\", printable=True)\n",
    "  path = F\"infattack/resnet-shadow_model_{i}.pt\"\n",
    "  torch.save({\n",
    "            'model_state_dict': shadow_model.state_dict(),\n",
    "            }, path)\n",
    "  print(f\"\\tTime taken: {time.process_time() - starttime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMoryz1taRIt"
   },
   "source": [
    "# Generating Attack Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "97YJeDb3aPFy",
    "outputId": "60196c3d-7bfa-4cd1-baf3-5c6c7e33628f"
   },
   "outputs": [],
   "source": [
    "# Create 100 attack model training sets, one for each class\n",
    "# These will be used to train 100 attack models, as per Shokri et al.\n",
    "\n",
    "sm = nn.Softmax()\n",
    "for c in range(100):\n",
    "  starttime = time.process_time()\n",
    "  attack_x = []\n",
    "  attack_y = []\n",
    "  # Generate attack training set for current class\n",
    "  for i, shadow_model_set in enumerate(shadow_models):\n",
    "    print(f\"\\rGenerating class {c} set from model {i}\", end=\"\")\n",
    "    shadow_model = shadow_model_set[0]\n",
    "    in_loader = torch.utils.data.DataLoader(shadow_datasets[i][0], batch_size=1)\n",
    "    for data, target in in_loader:\n",
    "      if target == c:\n",
    "        pred = shadow_model(data).view(100)\n",
    "        if torch.argmax(pred).item() == c:\n",
    "            attack_x.append(sm(pred))\n",
    "            attack_y.append(1)\n",
    "    out_loader = torch.utils.data.DataLoader(shadow_datasets[i][1], batch_size=1)\n",
    "    for data, target in out_loader:\n",
    "      if target == c:\n",
    "        pred = shadow_model(data).view(100)\n",
    "        attack_x.append(sm(pred))\n",
    "        attack_y.append(0)\n",
    "              \n",
    "  # Save datasets\n",
    "  tensor_x = torch.stack(attack_x)\n",
    "  tensor_y = torch.Tensor(attack_y)\n",
    "  xpath = f\"infattack/resnet_attack_x_{c}.pt\"\n",
    "  ypath = f\"infattack/resnet_attack_y_{c}.pt\"\n",
    "  torch.save(tensor_x, xpath)\n",
    "  torch.save(tensor_y, ypath)\n",
    "  tensor_x = torch.load(f\"infattack/resnet_attack_x_{c}.pt\")\n",
    "  tensor_y = torch.load(f\"infattack/resnet_attack_y_{c}.pt\")\n",
    "  print(torch.unique(tensor_y, return_counts=True)[1])\n",
    "  \n",
    "  # Create test and train dataloaders for attack dataset\n",
    "  attack_datasets = []\n",
    "  attack_datasets.append(torch.utils.data.TensorDataset(tensor_x, tensor_y))\n",
    "  attack_train, attack_test = torch.utils.data.random_split(\n",
    "    attack_datasets[0], [int(0.9*len(attack_datasets[0])), \n",
    "    len(attack_datasets[0]) - int(0.9*len(attack_datasets[0]))])\n",
    "  attackloader = torch.utils.data.DataLoader(attack_train, batch_size=batch_size, shuffle=True)\n",
    "  attacktester = torch.utils.data.DataLoader(attack_test, batch_size=batch_size, shuffle=True)\n",
    "  \n",
    "  # Create and train an attack model\n",
    "  attack_model, attack_optimizer = attack_model_fn()\n",
    "  for epoch in range(10):\n",
    "    trainattacker(attack_model, attack_optimizer, epoch, attackloader, printable=False)\n",
    "  print(fulltestattacker(attack_model, attacktester, dname=f\"Class {c}\"))\n",
    "  \n",
    "  # Save attack model\n",
    "  path = F\"infattack/resnet_attack_model_{c}.pt\"\n",
    "  torch.save({\n",
    "        'model_state_dict': attack_model.state_dict(),\n",
    "        }, path)\n",
    "  print(f\"Time taken: {time.process_time() - starttime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = targetclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant datasets and create test dataloader\n",
    "tensor_x = torch.load(f\"infattack/resnet_attack_x_{c}.pt\")\n",
    "tensor_y = torch.load(f\"infattack/resnet_attack_y_{c}.pt\")\n",
    "attack_datasets = []\n",
    "attack_datasets.append(torch.utils.data.TensorDataset(tensor_x, tensor_y))\n",
    "attacktester = torch.utils.data.DataLoader(attack_datasets[0], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant attack model\n",
    "path = F\"infattack/resnet_attack_model_{c}.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "attack_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulltestattacker(attack_model, attacktester, dname=f\"Class {c}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TargetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual target model to attack, trained in the same\n",
    "# way as the shadow models\n",
    "\n",
    "targetmodel, targetoptim = target_model_fn()\n",
    "trainingepochs = 10\n",
    "log_interval = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data, out_data = torch.utils.data.random_split(traindata, [int(len(traindata)/2), int(len(traindata)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "in_loader = torch.utils.data.DataLoader(in_data, batch_size=batch_size, shuffle=True)\n",
    "out_loader = torch.utils.data.DataLoader(out_data, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(1, trainingepochs+1):\n",
    "    print(f\"\\rEpoch {epoch}  \"  , end=\"\")\n",
    "    starttime = time.process_time()\n",
    "    steps.append(selectivetrain(targetmodel, targetoptim, epoch, in_loader, returnable=True))\n",
    "    print(f\"Time taken: {time.process_time() - starttime}\")\n",
    "test(targetmodel, testloader, dname=\"All data\", printable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = F\"infattack/cnn_target_trained.pt\"\n",
    "torch.save({\n",
    "            'model_state_dict': targetmodel.state_dict(),\n",
    "            'optimizer_state_dict': targetoptim.state_dict(),\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"infattack/cnn_batches.pkl\", \"wb\")\n",
    "pickle.dump(steps, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batches effected: {len(steps)}/{len(in_loader)*trainingepochs} = {100*len(steps)/(len(in_loader)*trainingepochs)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_loader = torch.utils.data.DataLoader(in_data, batch_size=1, shuffle=False)\n",
    "out_loader = torch.utils.data.DataLoader(out_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 attack model training sets, one for each class\n",
    "# These will be used to train 100 attack models, as per Shokri et al.\n",
    "# Currently configured to only produce for the target class\n",
    "attack_datasets = []\n",
    "sm = nn.Softmax()\n",
    "for c in range(targetclass, targetclass+1):\n",
    "    targetmodel.eval()\n",
    "    attackdata_x = []\n",
    "    attackdata_y = []\n",
    "    count = 0\n",
    "    print(f\"\\rGenerating class {c} set from target model\", end=\"\")\n",
    "    for data, target in in_loader:\n",
    "      if target == c:\n",
    "        pred = targetmodel(data).view(100)\n",
    "        if torch.argmax(pred).item() == c:\n",
    "            attackdata_x.append(data)\n",
    "            attackdata_y.append(1)\n",
    "            count += 1\n",
    "    for data, target in out_loader:\n",
    "      if target == c:\n",
    "        attackdata_x.append(data)\n",
    "        attackdata_y.append(0)\n",
    "        count += 1\n",
    "    attack_tensor_x = torch.stack(attackdata_x)\n",
    "    attack_tensor_y = torch.Tensor(attackdata_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_data = torch.utils.data.TensorDataset(attack_tensor_x, attack_tensor_y)\n",
    "atk_loader = torch.utils.data.DataLoader(atk_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testtargetmodel():\n",
    "    attack_datasets = []\n",
    "    sm = nn.Softmax()\n",
    "    for c in range(targetclass, targetclass+1):\n",
    "        targetmodel.eval()\n",
    "        attack_x = []\n",
    "        attack_y = []\n",
    "        for data, target in atk_loader:\n",
    "            data = data.reshape(1,3,32,32)\n",
    "            pred = targetmodel(data).view(100)\n",
    "            attack_x.append(sm(pred))\n",
    "            attack_y.append(target)\n",
    "        tensor_x = torch.stack(attack_x)\n",
    "        tensor_y = torch.Tensor(attack_y)\n",
    "        path = F\"infattack/resnet_attack_model_{c}.pt\"\n",
    "        checkpoint = torch.load(path)\n",
    "        attack_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        attack_datasets = []\n",
    "        attack_datasets.append(torch.utils.data.TensorDataset(tensor_x, tensor_y))\n",
    "        attacktester = torch.utils.data.DataLoader(attack_datasets[0], batch_size=batch_size, shuffle=True)\n",
    "        tp, tn, fp, fn = fulltestattacker(attack_model, attacktester, dname=f\"\\rclass {c}\")\n",
    "        recall = tp / (tp + fn)\n",
    "        print(f\"\\trecall: {recall}\")\n",
    "        missrate = fn / (fn + tp)\n",
    "#         print(f\"\\tmissrate: {missrate}\")\n",
    "        return recall, missrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_model, _ = attack_model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = F\"infattack/cnn_target_trained.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "targetmodel.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test amnesiac unlearning by testing membership inference\n",
    "# attack results on unprotected model, model after amnesiac\n",
    "# step, and after each epoch of further training\n",
    "\n",
    "recall = []\n",
    "missrate = []\n",
    "r, m = testtargetmodel()\n",
    "recall.append(r)\n",
    "missrate.append(m)\n",
    "for step in steps:\n",
    "    const = 1\n",
    "    with torch.no_grad():\n",
    "        state = targetmodel.state_dict()\n",
    "        for param_tensor in state:\n",
    "            if \"weight\" in param_tensor or \"bias\" in param_tensor:\n",
    "              state[param_tensor] = state[param_tensor] - const*step[param_tensor]\n",
    "    targetmodel.load_state_dict(state)\n",
    "r, m = testtargetmodel()\n",
    "recall.append(r)\n",
    "missrate.append(m)\n",
    "for epoch in range(5):\n",
    "    print(f\"\\rEpoch {epoch}  \"  , end=\"\")\n",
    "    starttime = time.process_time()\n",
    "    r, m = train2(targetmodel, targetoptim, epoch, nontarget_train_loader, printable=False)\n",
    "    recall = recall + r\n",
    "#     print(f\"Time taken: {time.process_time() - starttime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn mia datagen.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0024b395aaec4983a874c9c3f92af84e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c20e162490c433e83a3b82ac973a826",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d901d93b5f09484ea6c8ea6dfb9d8bcc",
      "value": 1
     }
    },
    "01824765ed9e4234adca0786222543ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c20e162490c433e83a3b82ac973a826": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c3251b4e7144cd3a5a92c2d8c00854a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0024b395aaec4983a874c9c3f92af84e",
       "IPY_MODEL_7d3fb8e3c37e46ebaaa1967e787e6a4e"
      ],
      "layout": "IPY_MODEL_01824765ed9e4234adca0786222543ff"
     }
    },
    "64d5070ef1d149a3af18a3b1fa0e3989": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d3fb8e3c37e46ebaaa1967e787e6a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64d5070ef1d149a3af18a3b1fa0e3989",
      "placeholder": "​",
      "style": "IPY_MODEL_aede667a8fa3464080eb6e029b271bcf",
      "value": " 169009152/? [00:30&lt;00:00, 16549487.85it/s]"
     }
    },
    "aede667a8fa3464080eb6e029b271bcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d901d93b5f09484ea6c8ea6dfb9d8bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}